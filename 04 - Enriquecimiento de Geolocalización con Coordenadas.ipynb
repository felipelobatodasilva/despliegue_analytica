{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar Bibliotecas\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 01:39:30 WARN Utils: Your hostname, felipe-Nitro-AN515-44 resolves to a loopback address: 127.0.1.1; using 192.168.15.10 instead (on interface wlp5s0)\n",
      "24/10/22 01:39:30 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/22 01:39:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/10/22 01:39:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/10/22 01:39:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/10/22 01:39:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/10/22 01:39:31 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "24/10/22 01:39:31 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Crea una sesión de Spark con el nombre de la aplicación \"Enriquecimiento de Geolocalización con Coordenadas\", \n",
    "habilitando el soporte para Hive, y la inicializa o reutiliza si ya existe\"\"\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Enriquecimiento de Geolocalización con Coordenadas\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo parquet de geolocalización de Correios\n",
    "geo = spark.read.parquet('/home/felipe/despliegue_analytica/files_parquet/geolocation_correios.parquet')\n",
    "# Leer el archivo parquet que contiene coordenadas geográficas\n",
    "coords_df = spark.read.parquet('/home/felipe/despliegue_analytica/files_parquet/geo_coords.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Se seleccionan las columnas 'cep_prefix', 'city', y 'uf' del DataFrame 'geo' y se eliminan los duplicados.\n",
    "Luego, se realiza una unión interna (inner join) con el DataFrame 'coords_df' que contiene las coordenadas, \n",
    "utilizando la columna 'cep_prefix' como clave de unión para garantizar que solo se mantengan los registros \n",
    "que coinciden en ambos DataFrames.\"\"\"\n",
    "geo_final = geo.select('cep_prefix','city', 'uf')\\\n",
    "   .dropDuplicates()\\\n",
    "   .join(\n",
    "        coords_df,\n",
    "        on=[\"cep_prefix\"],\n",
    "        how=\"inner\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+---+-----------+-----------+\n",
      "|cep_prefix|             city| uf|        lat|        lon|\n",
      "+----------+-----------------+---+-----------+-----------+\n",
      "|     64091|         teresina| PI| -5.0874608|-42.8049571|\n",
      "|     64250|  domingos mourao| PI| -4.2544732|-41.2722903|\n",
      "|     64250|  domingos mourao| PI| -4.2544732|-41.2722903|\n",
      "|     64400|         amarante| PI| -6.2459374|-42.8476238|\n",
      "|     64420|       palmeirais| PI| -5.9761427|-43.0637158|\n",
      "|     64607|            picos| PI| -7.0823544|-41.4685053|\n",
      "|     66912|            belem| PA|   -1.45056|-48.4682453|\n",
      "|     66912|            belem| PA|   -1.45056|-48.4682453|\n",
      "|     68627|      paragominas| PA|   -2.99564|-47.3548942|\n",
      "|     69042|           manaus| AM| -3.1316333|-59.9825041|\n",
      "|     69928|placido de castro| AC|-10.3239154|-67.1824196|\n",
      "|     69928|placido de castro| AC|-10.3239154|-67.1824196|\n",
      "|     70719|         brasilia| DF|-15.7934036|-47.8823172|\n",
      "|     70719|         brasilia| DF|-15.7934036|-47.8823172|\n",
      "|     70864|         brasilia| DF|-15.7934036|-47.8823172|\n",
      "|     70864|         brasilia| DF|-15.7934036|-47.8823172|\n",
      "|     70856|         brasília| DF|-15.7934036|-47.8823172|\n",
      "|     70856|         brasília| DF|-15.7934036|-47.8823172|\n",
      "|     71050|            guara| DF|-15.7934036|-47.8823172|\n",
      "|     71050|            guara| DF|-15.8235629|-47.9768165|\n",
      "+----------+-----------------+---+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "geo_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Se convierten las columnas 'lat' y 'lon' del DataFrame 'geo_final' al tipo de dato 'double' \n",
    "para garantizar mayor precisión al trabajar con coordenadas geográficas, asegurando que los valores \n",
    "de latitud y longitud se almacenen como números de punto flotante.\"\"\"\n",
    "geo_final = geo_final\\\n",
    ".withColumn('lat', F.col('lat').cast('double'))\\\n",
    ".withColumn('lon', F.col('lon').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cep_prefix: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- uf: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar el esquema del DataFrame final\n",
    "geo_final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Escribir el DataFrame resultante en un archivo parquet con las coordenadas de geolocalización\n",
    "geo_final.write.parquet(f'/home/felipe/despliegue_analytica/files_parquet/geolocation_correios_coords.parquet', mode='overwrite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
