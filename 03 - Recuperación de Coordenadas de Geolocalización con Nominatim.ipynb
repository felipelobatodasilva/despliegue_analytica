{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar Bibliotecas\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from geopy.geocoders import Nominatim\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/22 01:27:32 WARN Utils: Your hostname, felipe-Nitro-AN515-44 resolves to a loopback address: 127.0.1.1; using 192.168.15.10 instead (on interface wlp5s0)\n",
      "24/10/22 01:27:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/10/22 01:27:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/10/22 01:27:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/10/22 01:27:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "24/10/22 01:27:34 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "24/10/22 01:27:34 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Crea una sesión de Spark con el nombre de la aplicación \"Recuperación de Coordenadas de Geolocalización con Nominatim\", \n",
    "habilitando el soporte para Hive, y la inicializa o reutiliza si ya existe\"\"\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Recuperación de Coordenadas de Geolocalización con Nominatim\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "geo = spark.read.parquet('/home/felipe/despliegue_analytica/files_parquet/geolocation_correios.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Seleccionar ciudades y estados distintos\n",
    "cidades_ufs = geo.select('cep_prefix', 'city', 'uf').distinct().collect()\n",
    "qtde_cidades_ufs = geo.select('cep_prefix', 'city', 'uf').distinct().count()\n",
    "\n",
    "coords = []\n",
    "counter = 0\n",
    "\n",
    "# Realiza consultas utilizando Geopy para obtener las coordenadas de cada conjunto de ciudad y estado\n",
    "for linha in cidades_ufs:\n",
    "    print(f\"{counter}º Consulta de {qtde_cidades_ufs}\", end=\"\\r\")\n",
    "    try:\n",
    "        # Inicializa Nominatim con un tiempo de espera de 30 segundos\n",
    "        geolocator = Nominatim(user_agent=\"test_app\", timeout=30)\n",
    "        # Intenta obtener las coordenadas de la ciudad y el estado\n",
    "        location = geolocator.geocode(f'{linha[\"city\"]}, {linha[\"uf\"]}')\n",
    "        if location:\n",
    "            # Si se encuentra la ubicación, agrega la coordenada a la lista\n",
    "            coords.append([linha['cep_prefix'], location.latitude, location.longitude])\n",
    "            counter += 1\n",
    "        else:\n",
    "            print(f\"Ubicación no encontrada para {linha['city']}, {linha['uf']}\")\n",
    "        # Espera 1 segundo entre consultas para no sobrecargar el servicio\n",
    "        time.sleep(1)\n",
    "    except Exception as e:\n",
    "        # Captura y muestra cualquier error ocurrido\n",
    "        print(f\"Error: {e}\")\n",
    "        pass\n",
    "\n",
    "# Verifica si la lista `coords` contiene datos antes de crear el DataFrame\n",
    "if len(coords) > 0:\n",
    "    # Crea un DataFrame con las coordenadas de cada cep\n",
    "    coords_df = spark.createDataFrame(coords, schema=[\"cep_prefix\", \"lat\", \"lon\"])\n",
    "    \n",
    "    # Guarda el DataFrame en formato Parquet con el nombre 'geo_coords.parquet'\n",
    "    coords_df.write.parquet(\"/home/felipe/despliegue_analytica/files_parquet/geo_coords.parquet\")\n",
    "    print(\"DataFrame guardado como geo_coords.parquet\")\n",
    "else:\n",
    "    print(\"No se obtuvieron coordenadas.\")\n",
    "\n",
    "duration = time.time() - start_time\n",
    "print(f\"Tiempo total: {duration/60} minutos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-despliegue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
